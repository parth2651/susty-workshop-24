import json
import time
import boto3
## return a vector embedding for given text using AWS Bedrock Titan embedding API
## bedrock_runtime should be defined

def addconversation(conversation,question,answer):
    conversation.appdend({"role": "user", "content": question})
    conversation.appdend({"role": "assistant", "content": answer})
    return conversation


def llm_embed(bedrock_runtime,input_text):

    model_id = "amazon.titan-embed-text-v1"
    # body = json.dumps({
    #   "inputText": text
    #     })
    # kwargs = {
    #     "modelId": model,
    #     "contentType": "application/json",
    #     "accept": "*/*",
    #     "body": body
    #   }
    # response = bedrock_runtime.invoke_model(**kwargs)
    # response_body = json.loads(response.get('body').read())
    # response = response_body.get('embedding')
    # return response
    # Create request body.
    body = json.dumps({
        "inputText": input_text,
    })
    accept = "application/json"
    content_type = "application/json"
    #bedrock_runtime = boto3.client(service_name='bedrock-runtime')
    response = bedrock_runtime.invoke_model(
        body=body, modelId=model_id, accept=accept, contentType=content_type
    )
   
    response_body = json.loads(response.get('body').read())
    
    return response_body['embedding']



# ##### Interact with a large language model (LLM) to generate text 
# # based on a prompt.
# #
# # Arguments:
# #   prompt: The text prompt to provide to the LLM.
# #   llm_type: The name of the LLM to use'. 
# #
# # Returns:
# #   The text generated by the LLM in response to the prompt.
# #   
# # This function:
# # 1. Prints the llm_type for debugging.
# # 2. Formats the prompt into the JSON payload expected by each LLM API.
# # 3. Specifies the parameters for text generation like max tokens, temp.
# # 4. Calls the Bedrock client to invoke the LLM model API. 
# # 5. Parses the response to extract the generated text.
# # 6. Returns the generated text string.


def interactWithLLM(bedrock_runtime,prompt,image,llm_type):
	
        # if llm_type == 'anthropic.claude-3-sonnet':
        #     print("**THE LLM TYPE IS -->" + llm_type)
        if llm_type == 'anthropic.claude-3-sonnet':
            modelId = 'anthropic.claude-3-sonnet-20240229-v1:0' # change this to use a different version from the model provider
        if llm_type == 'anthropic.claude-3-haiku':
            modelId = 'anthropic.claude-3-haiku-20240307-v1:0' # change this to use a different version from the model provider

        body = json.dumps({
                      "anthropic_version": "bedrock-2023-05-31",
                      "max_tokens": 1000,
                      "messages": [
                        {
                          "role": "user",
                          "content": [
                            {
                              "type": "image",
                              "source": {
                                "type": "base64",
                                "media_type": "image/jpeg",
                                "data": image
                              }
                            },
                            {
                              "type": "text",
                              "text": prompt
                            }
                          ]
                        }
                      ]
                    }) 
        #modelId = 'anthropic.claude-3-sonnet-20240229-v1:0' # change this to use a different version from the model provider
        accept = 'application/json'
        contentType = 'application/json'
        start_time = time.time()
        response = bedrock_runtime.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)
         # Record the end time
        end_time = time.time()

        # Calculate the runtime
        runtime = end_time - start_time
        #print(f"The runtime of the invoke_model was {runtime:.2f} seconds.")
        
        response_body = json.loads(response.get('body').read())
        response_text = response_body.get('content')[0]['text']

        return response_text

def interactWithLLMText(bedrock_runtime,prompt,text,llm_type):
	
    # if llm_type == 'anthropic.claude-3-sonnet':
    #     print("**THE LLM TYPE IS -->" + llm_type)
    modelId="anthropic.claude-3-haiku-20240307-v1:0" # setting haiku as default
    if llm_type == 'anthropic.claude-3-sonnet':
        modelId = 'anthropic.claude-3-sonnet-20240229-v1:0' # change this to use a different version from the model provider
    if llm_type == 'anthropic.claude-3-haiku':
        modelId = 'anthropic.claude-3-haiku-20240307-v1:0' # change this to use a different version from the model provider

    body = json.dumps({
                      "anthropic_version": "bedrock-2023-05-31",
                      "max_tokens": 100000,
                      "messages": [
                        {
                          "role": "user",
                          "content": [
                            # {
                            #   "type": "image",
                            #   "source": {
                            #     "type": "base64",
                            #     "media_type": "image/jpeg",
                            #     "data": image
                            #   }
                            # },
                            {
                              "type": "text",
                              "text": prompt
                            }
                          ]
                        }
                      ]
                    }) 
#    modelId = 'anthropic.claude-3-sonnet-20240229-v1:0' # change this to use a different version from the model provider
    accept = 'application/json'
    contentType = 'application/json'
    start_time = time.time()
    response = bedrock_runtime.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)
     # Record the end time
    end_time = time.time()

    # Calculate the runtime
    runtime = end_time - start_time
    #print(f"The runtime of the invoke_model was {runtime:.2f} seconds.")

    response_body = json.loads(response.get('body').read())
    response_text = response_body.get('content')[0]['text']

    return response_text

def interactWithLLMTextfromAnthropic(bedrock_runtime,prompt,text,llm_type,systemprompt):
	
    # if llm_type == 'anthropic.claude-3-sonnet':
    #     print("**THE LLM TYPE IS -->" + llm_type)
    modelId="anthropic.claude-3-haiku-20240307-v1:0" # setting haiku as default
    if llm_type == 'anthropic.claude-3-sonnet':
        modelId = 'anthropic.claude-3-sonnet-20240229-v1:0' # change this to use a different version from the model provider
    if llm_type == 'anthropic.claude-3-haiku':
        modelId = 'anthropic.claude-3-haiku-20240307-v1:0' # change this to use a different version from the model provider

    body = json.dumps({
                      "anthropic_version": "bedrock-2023-05-31",
                      "max_tokens": 200000,
                      "temperature": 0.2,
                      #"top_p": float,
                      "top_k": 500,
                      "system": systemprompt,
                      "messages": [
                        {
                          "role": "user",
                          "content": [
                            # {
                            #   "type": "image",
                            #   "source": {
                            #     "type": "base64",
                            #     "media_type": "image/jpeg",
                            #     "data": image
                            #   }
                            # },
                            {
                              "type": "text",
                              "text": prompt
                            }
                          ]
                        }
                      ]
                    }) 
#    modelId = 'anthropic.claude-3-sonnet-20240229-v1:0' # change this to use a different version from the model provider
    accept = 'application/json'
    contentType = 'application/json'
    start_time = time.time()
    response = bedrock_runtime.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)
     # Record the end time
    end_time = time.time()

    # Calculate the runtime
    runtime = end_time - start_time
    #print(f"The runtime of the invoke_model was {runtime:.2f} seconds.")

    response_body = json.loads(response.get('body').read())
    response_text = response_body.get('content')[0]['text']

    return response_text